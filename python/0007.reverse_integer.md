# 7. Reverse Integer

```python
class Solution:
    def reverse(self, x: int) -> int:
        INT_MAX, INT_MIN = 2**31 - 1, -2**31
        
        rev = 0
        sign = 1 if x > 0 else -1
        x = abs(x)
        
        while x:
            last = x % 10
            ## Check if int overflow will happen before storing the value
            if rev > (INT_MAX - last) // 10:
                return 0
            rev = rev * 10 + last
            x //= 10
        
        return rev * sign
```

The question strictly specifies the following:

> Assume the environment does not allow you to store 64-bit integers (signed or unsigned).

Hence, the condition `if rev > (INT_MAX - last) // 10:` is critical to the 
problem because it makes sure that we are not saving any integer value which may
exceed the `INT_MAX` value.


**Time Complexity:** O(log(x))   
**Space Complexity:** O(1)


---

## Understanding log10(x) Time Complexity


### ğŸ”¹ Step 1: What is a logarithm, really?
- The logarithm is the **inverse of exponentiation**.  
  $$\log_b(n) = k \quad \iff \quad b^k = n$$
- Example:  
  $$\log_2(8) = 3 \quad \text{because } 2^3 = 8.$$

So, a logarithm answers the question:  
**â€œHow many times do I multiply/divide by the base until I reach the number?â€**

---

### ğŸ”¹ Step 2: Why does `log` show up in algorithms?
Logarithms naturally appear when:
1. **You repeatedly divide (or multiply) a problem size by a constant factor.**
   - Binary search: divide by 2 each step â†’ $$\log_2(n)$$ steps.
   - Balanced trees: height is $$\log(n)$$ because each level halves the remaining nodes.
2. **You measure something in terms of digits.**
   - Number of digits in base 10: $$\lfloor \log_{10}(n) \rfloor + 1$$
   - Number of bits in binary: $$\lfloor \log_{2}(n) \rfloor + 1$$

So, logarithms are a way of counting **â€œhow many times can I shrink this thing until itâ€™s gone?â€**

---

### ğŸ”¹ Step 3: Applying this to your reverse-integer code
Look at the loop:

```python
while x:
    last = x % 10
    rev = rev * 10 + last
    x //= 10
```

- Each iteration does `x //= 10`, i.e., divides `x` by 10.
- How many times can you divide `x` by 10 before it becomes 0?  
  â†’ Exactly the number of **digits** in `x`.

And the number of digits in `x` is:
$$\text{digits}(x) = \lfloor \log_{10}(x) \rfloor + 1$$

So the loop runs **O(logâ‚â‚€(x))** times.

---

### ğŸ”¹ Step 4: Why do we write `O(log x)` instead of `O(logâ‚â‚€ x)`?
- In Big-O, the **base of the logarithm doesnâ€™t matter** because:
  $$\log_a(n) = \frac{\log_b(n)}{\log_b(a)}$$
  â†’ They differ only by a constant factor, and Big-O ignores constants.
- So whether itâ€™s base 2, base 10, or base e, we just write `O(log n)`.

---

### ğŸ”¹ Step 5: Comparing with Binary Search
- **Binary Search**: divide by 2 each step â†’ $$\log_2(n)$$ steps.
- **Reverse Integer**: divide by 10 each step â†’ $$\log_{10}(x)$$ steps.
- Both are logarithmic, just with different bases.

---

### ğŸ”¹ Step 6: Interview-level explanation
If asked in an interview:
- **Q: Why is the time complexity `O(log x)`?**  
  A: Because the loop runs once per digit of `x`. The number of digits in `x` is proportional to $$\log_{10}(x)$$. Each iteration does constant work, so total time is `O(log x)`.

- **Q: Why not `O(n)`?**  
  A: Because `n` usually refers to input size (like array length). Here, the input is a single integer, and the relevant measure of size is its number of digits, which grows logarithmically with the numeric value.

- **Q: What if `x` has k digits?**  
  A: Then the loop runs `k` times. Since `k = O(log x)`, thatâ€™s the complexity.

---

### ğŸ”¹ Step 7: Advanced intuition
Think of **input size** in two ways:
1. **Value size**: The number itself (`x`).
2. **Representation size**: How many symbols (digits/bits) are needed to write it.

- For integers, the representation size is **logarithmic in the value**.  
  Example:  
  - `x = 123` â†’ 3 digits â†’ loop runs 3 times.  
  - `x = 1,000,000` â†’ 7 digits â†’ loop runs 7 times.  
  - Notice how the loop count grows much slower than the number itself.

This is why logarithms are the natural language of time complexity when dealing with digits, bits, or repeated division.

---

âœ… **Summary you can use in interviews:**
- Logarithms appear in time complexity when the problem size shrinks by a constant factor each step (like binary search) or when the number of digits/bits matters (like reversing an integer).  
- In the reverse-integer problem, the loop runs once per digit, and the number of digits in `x` is $$O(\log x)$$.  
- The base of the log doesnâ€™t matter in Big-O, so we just write `O(log x)`.

